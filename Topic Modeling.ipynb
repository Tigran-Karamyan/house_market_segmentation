{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install gensim\n",
    "# ! pip install spacy \n",
    "# ! pip install wordcloud\n",
    "# ! pip install pyLDAvis\n",
    "# ! pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from num2words import num2words\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#LdaVis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#nltk\n",
    "import nltk\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fake_ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/appartment_descriptions_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Na's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i,\"--------->\", df[i].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    A = df.loc[:col].isnull().sum()\n",
    "print(A)\n",
    "print(\"Text length =\", len(df.descr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh=3, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_rooms'] = df.rooms\n",
    "df['num_floor'] = df.floor.apply(lambda x: x.replace('floor', ''))\n",
    "df['num_price'] = df.price.apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_numbers_to_oridinals(text, category=None, wordify=True):\n",
    "    if wordify:\n",
    "        return re.sub(r\"(\\d+)\", lambda x: num2words(x.group(0), category), text)\n",
    "    return re.sub(r\"(\\d+)\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.addr = df.addr.apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "df.rooms = df.rooms.apply(lambda x: replace_numbers_to_oridinals(x))\n",
    "df.ruler = df.ruler.apply(lambda x: replace_numbers_to_oridinals(x))\n",
    "df.ruler = df.ruler.apply(lambda x: x.replace('mtwo','square metres'))\n",
    "df.floor = df.floor.apply(lambda x: replace_numbers_to_oridinals(x, category='ordinal'))\n",
    "df.floor = df.floor.apply(lambda x: x.replace(',', ' '))\n",
    "df.price = df.price.apply(lambda x: x.replace(',', ''))\n",
    "df.price = df.price.apply(lambda x: x.replace(' ', ''))\n",
    "df.descr = df.descr.apply(lambda x: replace_numbers_to_oridinals(x, wordify=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stemmer = SnowballStemmer('english') \n",
    "stops = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def stemming_(Sentence):\n",
    "    cleaned_ver = [stemmer.stem(word) for word in word_tokenize(Sentence) if word not in stops and word.isalnum()]\n",
    "    return ' '.join(cleaned_ver)\n",
    "\n",
    "def lemmatize_(Sentence):\n",
    "    cleaned_ver = [lemmatizer.lemmatize(word) for word in word_tokenize(Sentence) if word not in stops and word.isalnum()]\n",
    "    return ' '.join(cleaned_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df[\"stem_text\"] = 0\n",
    "df[\"lemm_text\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.stem_text = df.descr.apply(lambda x: stemming_(x))\n",
    "df.lemm_text = df.descr.apply(lambda y: lemmatize_(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[[\"descr\", \"stem_text\", \"lemm_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df.addr + \" \" + df.rooms + \" \" + df.ruler + \" \" + df.floor + \" \" + df.price + \" \" + df.lemm_text + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.text.values.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "long_string2 = ','.join(list(df.text.values))\n",
    "wordcloud = WordCloud(background_color = \"white\", width = 500, height = 500, max_words = 5000, contour_width = 20)\n",
    "wordcloud.generate(long_string2)\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram and Trigram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_15_most_common_words(count_data, count_vectorizer):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts += t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:15]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='15 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Unigram_vectorizer = CountVectorizer()\n",
    "Trigram_vectorizer = CountVectorizer(ngram_range = (3, 3),\n",
    "                                     token_pattern = r'\\b\\w+\\b',\n",
    "                                     min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Unigram_data = Unigram_vectorizer.fit_transform(df.text)\n",
    "Trigram_data = Trigram_vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer - How it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Unigram_vectorizer.get_feature_names()[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total = np.zeros(len(Unigram_vectorizer.get_feature_names()))\n",
    "print(total, len(total))\n",
    "for t in Unigram_data:\n",
    "#     print(t)\n",
    "#     print(len(t.toarray()[0]))\n",
    "    total += t.toarray()[0]\n",
    "print(total, len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_15_most_common_words(Unigram_data, Unigram_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_15_most_common_words(Trigram_data, Trigram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF on Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "TF_Vectorizer = TfidfVectorizer(ngram_range = (1, 3),\n",
    "                                    token_pattern = r'\\b\\w+\\b', min_df=1)\n",
    "Data = TF_Vectorizer.fit_transform(df.text)\n",
    "plot_15_most_common_words(Data, TF_Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    \n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "number_topics = 3\n",
    "number_words = 4\n",
    "\n",
    "lda = LDA(number_topics, n_jobs = None)\n",
    "lda.fit(Data)\n",
    "\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, TF_Vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Detailed LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                                num_topics = num_topics,\n",
    "                                                id2word=dictionary,\n",
    "                                                passes=25,\n",
    "                                                alpha='auto',\n",
    "                                                update_every=0,\n",
    "                                                random_state=76)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=text_data, dictionary=dictionary, corpus=corpus, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text_data = [word_tokenize(i) for i in df.text]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the optimal number of topics for LDA?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary,\n",
    "                                                        corpus=corpus, texts=text_data, limit=15, start=2, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall coherence score of a topic is the average of the distances between words.\n",
    "\n",
    "0.3 is bad\n",
    "\n",
    "0.4 is low (probably have the wrong number of topics )\n",
    "\n",
    "0.55 is okay\n",
    "\n",
    "0.65 might be as good as it is going to get\n",
    "\n",
    "0.7 is nice\n",
    "\n",
    "0.8 is unlikely and\n",
    "\n",
    "0.9 is probably wrong"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = range(2, 15, 1)\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 2, id2word=dictionary, passes=20, random_state=76)\n",
    "ldamodel.save('model.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret this?\n",
    "\n",
    "Topic 0 is a represented as '0.014*\"The\" + 0.014*\"said\" + 0.006*\"police\" + 0.006*\"law\" + 0.005*\"case\"')\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model.gensim')\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how to infer pyLDAvisâ€™s output?\n",
    "\n",
    "* Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "* A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "* A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "* Salient keywords form the selected topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def retrive_needed_words(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] in ['NN', 'JJ', 'VB', 'RB']\n",
    "#                                           [ 'JJR', 'JJS', 'VBD', 'VBG','VBN', 'VBP', 'VBZ', 'NNP', 'NNPS', 'NNS',  'RBR', 'RBS']\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word.lower() for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simplified_text_data = list()\n",
    "for text in df.text:\n",
    "    simplified_text = retrive_needed_words(text)\n",
    "    simplified_text_data.append(simplified_text)\n",
    "    print(\"Normal text: \", text, '\\nSimplified text: ', simplified_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_Vectorizer_for_modelling = TfidfVectorizer(ngram_range = (1, 3))\n",
    "Data = TF_Vectorizer_for_modelling.fit_transform(simplified_text_data)\n",
    "words = TF_Vectorizer_for_modelling.get_feature_names()\n",
    "total_counts = np.zeros(len(words))\n",
    "for t in Data:\n",
    "    total_counts += t.toarray()[0]\n",
    "\n",
    "count_dict = (zip(words, total_counts))\n",
    "count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[:25]\n",
    "words = [w[0] for w in count_dict]\n",
    "counts = [round(float(w[1]),2) for w in count_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequent_words = {k:v for (k,v) in zip(words, counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data = [word_tokenize(i) for i in simplified_text_data]\n",
    "\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "pickle.dump(corpus, open('gensim/corpus.pkl', 'wb'))\n",
    "dictionary.save('gensim/dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary,\n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=text_data, \n",
    "                                                        limit=6, \n",
    "                                                        start=2, \n",
    "                                                        step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = range(2, 15, 1)\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, alpha='auto', passes=30, random_state=76)\n",
    "ldamodel.save('gensim/model.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dictionary = gensim.corpora.Dictionary.load('gensim/dictionary.gensim')\n",
    "corpus = pickle.load(open('gensim/corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('gensim/model.gensim')\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topic in Each Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamodel, corpus=corpus):\n",
    "\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['dominant_topic', 'percentage_contribution', 'topic_keywords']\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index(drop=True)\n",
    "df_dominant_topic.columns = ['dominant_topic', 'topic_percentage_contrib', 'keywords']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.sort_values(['dominant_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.merge(df, df_dominant_topic, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(\"data/appartment_descriptions_eng_with_coordinates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
