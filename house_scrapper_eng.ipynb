{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561f978",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install translate\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1feba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_appartment_urls = list()\n",
    "download_urls = [\n",
    "    # Sale\n",
    "    'https://www.estate.am/en/apartments-for-sale-s406442?page=',\n",
    "    # Monthly Rent\n",
    "    'https://www.estate.am/en/apartments-rentals-s131146?page=',\n",
    "    # Daily Rent\n",
    "    'https://www.estate.am/en/daily-rental-apartments-s299123?page='\n",
    "]\n",
    "\n",
    "try:\n",
    "    old_urls = pd.read_csv('data/urls_eng.csv')\n",
    "    count = old_urls.row.values.tolist()[-1]\n",
    "    for url in download_urls:\n",
    "        for pages in range(1, 10000):\n",
    "            URL = url + f'{pages}'\n",
    "            page = requests.get(URL)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            house_element = soup.find_all(\"td\", class_=\"last\")\n",
    "            if len(house_element) == 0:\n",
    "                break\n",
    "            for house_url in house_element:\n",
    "                get_house_url = house_url.find(\"a\", href=True)\n",
    "                new_url = \"https://www.estate.am/en\" + get_house_url['href']\n",
    "                if new_url not in old_urls.url.values.tolist():\n",
    "                    count+=1\n",
    "                    all_appartment_urls.append([count, new_url])\n",
    "    print('Fetching new Urls... ')\n",
    "    print('New Urls: ', len(all_appartment_urls), 'Existing Urls: ', old_urls.shape[0])\n",
    "    if len(all_appartment_urls) != 0:\n",
    "        with open('data/urls_eng.csv', 'a+', encoding=\"UTF-8\", newline='') as f_object:\n",
    "            writer_object = csv.writer(f_object)\n",
    "            for row in all_appartment_urls:\n",
    "                writer_object.writerow(row)\n",
    "    urls = pd.read_csv('data/urls_eng.csv')\n",
    "    print(\"Updated Urls DB: \", urls.shape[0])\n",
    "                \n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    print(\"Creating new file ...\")\n",
    "    count = 0\n",
    "    for url in download_urls:\n",
    "        for pages in range(1, 10000):\n",
    "            URL = url + f'{pages}'\n",
    "            page = requests.get(URL)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            house_element = soup.find_all(\"td\", class_=\"last\")\n",
    "            if len(house_element) == 0:\n",
    "                break\n",
    "            for house_url in house_element:\n",
    "                get_house_url = house_url.find(\"a\", href=True)\n",
    "                new_url = \"https://www.estate.am/en\" + get_house_url['href']\n",
    "                count+=1\n",
    "                all_appartment_urls.append([count, new_url])\n",
    "    print(\"Updated Urls DB: \", len(all_appartment_urls))\n",
    "    urls = pd.DataFrame(all_appartment_urls, columns=['row', 'url'])\n",
    "    urls.to_csv('data/urls_eng.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96e7a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "appartments = list()\n",
    "try:\n",
    "    appartment_db = pd.read_csv('data/appartment_descriptions_eng.csv')\n",
    "    count = old_urls.row.values.tolist()[-1]\n",
    "    print('Fetching new Appartments ...')\n",
    "    new_appartments = urls.shape[0] - appartment_db.shape[0]\n",
    "    if new_appartments > 0:\n",
    "        for apartment in urls.url.tolist()[-new_appartments:]:\n",
    "            if apartment not in old_urls.url.values.tolist():\n",
    "                page = requests.get(apartment)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                time.sleep(1.5)\n",
    "\n",
    "                appartments.append(\n",
    "                    [\n",
    "                        soup.find(\"strong\", class_='addr').text, \n",
    "                        soup.find(\"span\", class_='rooms').text,\n",
    "                        soup.find(\"span\", class_='ruler').text,\n",
    "                        re.sub('\\s+', ' ', soup.find(\"span\", class_='floor').text),\n",
    "                        re.sub('\\s+', ' ', soup.find(\"div\", class_='price-w').text),\n",
    "                        re.sub('\\s+', ' ', soup.find(\"p\").text),\n",
    "                        soup.find('div', id='yandex-map')['data-x'],\n",
    "                        soup.find('div', id='yandex-map')['data-y']\n",
    "\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                count+=1\n",
    "                print(count)\n",
    "    print('New Appartments: ', len(appartments), 'Existing Appartments: ', appartment_db.shape[0])\n",
    "    if len(appartments) != 0:\n",
    "        with open('data/appartment_descriptions_eng.csv', 'a+', encoding=\"UTF-8\", newline='') as f_object:\n",
    "            writer_object = csv.writer(f_object)\n",
    "            for row in appartments:\n",
    "                writer_object.writerow(row)\n",
    "    df_appartments = pd.read_csv('data/appartment_descriptions_eng.csv')\n",
    "    print(\"Updated Appartment DB: \", df_appartments.shape[0])\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating Appartment DB ...\")\n",
    "    count = 0\n",
    "    for apartment in urls.url.values.tolist():\n",
    "        page = requests.get(apartment)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        appartments.append(\n",
    "            {\n",
    "                'addr': soup.find(\"strong\", class_='addr').text, \n",
    "                'rooms': soup.find(\"span\", class_='rooms').text,\n",
    "                'ruler': soup.find(\"span\", class_='ruler').text,\n",
    "                'floor': re.sub('\\s+', ' ', soup.find(\"span\", class_='floor').text),\n",
    "                'price': re.sub('\\s+', ' ', soup.find(\"div\", class_='price-w').text),\n",
    "                'descr': re.sub('\\s+', ' ', soup.find(\"p\").text),\n",
    "                'lat': soup.find('div', id='yandex-map')['data-x'],\n",
    "                'lon': soup.find('div', id='yandex-map')['data-y']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        count+=1\n",
    "        print(count)\n",
    "    df_appartments = pd.DataFrame(appartments)\n",
    "    print('Appartment DB: ', count)\n",
    "    df_appartments.to_csv('data/appartment_descriptions_eng.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
